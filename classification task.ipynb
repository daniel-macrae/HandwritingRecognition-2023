{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the letters for classification \n",
    "Later should use the segmented created pages to pretrain the model\n",
    "And the actual dataset to fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.loadHabbakuk import habbakukLettersDataset\n",
    "from data_management.datasetSplitter import datasetSplitter\n",
    "from segment import Segment\n",
    "\n",
    "AugmentedFolder = \"Data/Habbakuk_Letters\"\n",
    "\n",
    "# options on whether or not you want the labels and/or the BBs (both false also works)\n",
    "Aug_dataset = habbakukLettersDataset(folder_path=AugmentedFolder)\n",
    "BATCH_SIZE = 16\n",
    "train_loader, validation_loader = datasetSplitter(Aug_dataset, batch_size=BATCH_SIZE, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom data_management.loadDSSCharacters import dssLettersDataset\\nfrom data_management.datasetSplitter import datasetSplitter, collate_fn\\n\\nfrom torch.utils.data import DataLoader\\n\\nBATCH_SIZE = 16\\nshuffle_loaders = True\\n\\n\\ndataSource = \"monkbrill2\" # name of the folder, works with \"monkbrill\" as well\\n\\ntrain_folder = \"Data/dssLetters/train/\"\\nval_folder = \"Data/dssLetters/validation/\"\\ntest_folder = \"Data/dssLetters/test/\"\\n\\ntrain_dataset = dssLettersDataset(folder_path=train_folder)\\nval_dataset = dssLettersDataset(folder_path=val_folder)\\ntest_dataset = dssLettersDataset(folder_path=train_folder)\\n\\n\\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\\nvalidation_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\\n\\n\\ncharacterClasses = train_dataset.characterClasses\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monkbril2 train, test, validation sets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from data_management.loadDSSCharacters import dssLettersDataset\n",
    "from data_management.datasetSplitter import datasetSplitter, collate_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "shuffle_loaders = True\n",
    "\n",
    "\n",
    "dataSource = \"monkbrill2\" # name of the folder, works with \"monkbrill\" as well\n",
    "\n",
    "train_folder = \"Data/dssLetters/train/\"\n",
    "val_folder = \"Data/dssLetters/validation/\"\n",
    "test_folder = \"Data/dssLetters/test/\"\n",
    "\n",
    "train_dataset = dssLettersDataset(folder_path=train_folder)\n",
    "val_dataset = dssLettersDataset(folder_path=val_folder)\n",
    "test_dataset = dssLettersDataset(folder_path=train_folder)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "\n",
    "\n",
    "characterClasses = train_dataset.characterClasses\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = next(iter(train_loader))\n",
    "input[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later try to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#segment = Segment()\n",
    "#segment.segment_characters(real_data=True, generated_data=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arquitecture has been decided very arbitrarily for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharacterCNN(nn.Module):\n",
    "    def __init__(self, numChannels = 1, classes = 27, BATCH_SIZE = 16, dropout_rate = 0.5):\n",
    "        super(CharacterCNN, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        self.conv1 = nn.Conv2d(numChannels, 32, kernel_size=5, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, BATCH_SIZE, kernel_size=5, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear( BATCH_SIZE *12* 12, 128) #this shoulf be BATCH_SIZE *12* 12 but that doesnt work\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "        self.relu = nn.ReLU() # could also just use nn.functional.relu\n",
    "        #self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.conv1 = nn.Conv2d(numChannels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, BATCH_SIZE, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear( BATCH_SIZE *12* 12, 64) #this shoulf be BATCH_SIZE *12* 12 but that doesnt work\n",
    "        self.fc2 = nn.Linear(64, classes)\n",
    "        self.relu = nn.ReLU() # could also just use nn.functional.relu\n",
    "        #self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "\n",
    " \n",
    "    # Using nn.functional provides a more concise syntax since it directly applies the operations as functions,\n",
    "    # while using the corresponding layers from torch.nn allows for explicit control over the layers used in the network\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        #x = torch.flatten(x, 1)\n",
    "        x = x.view(x.size(0), -1) #same as flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        #output = self.logSoftmax(output)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CharacterCNN()\n",
    "\n",
    "# Print the model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") # ??¿¿\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# construct the argument parser and parse the arguments\\nmodel_dir = \"Model1\"\\nos.makedirs(model_dir, exist_ok = True)\\n\\nap = argparse.ArgumentParser()\\nap.add_argument(\"-m\", \"--model\", type=str, required=True,\\n\\thelp=model_dir)\\nap.add_argument(\"-p\", \"--plot\", type=str, required=True,\\n\\thelp=model_dir)\\nargs = vars(ap.parse_args())\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# construct the argument parser and parse the arguments\n",
    "model_dir = \"Model1\"\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n",
    "\thelp=model_dir)\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, required=True,\n",
    "\thelp=model_dir)\n",
    "args = vars(ap.parse_args())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
    "valSteps = len(validation_loader.dataset) // BATCH_SIZE\n",
    "\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the  model\n",
    "\n",
    "model = CharacterCNN().to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss() #When we combine the nn.NLLoss class with LogSoftmax in our model definition, we arrive at categorical cross-entropy loss\n",
    "#  (which is the equivalent to training a model with an output Linear layer and an nn.CrossEntropyLoss loss).\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "# measure how long training is going to take\n",
    "startTime = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/100\n",
      "Train loss: 2.951814, Train accuracy: 0.0222\n",
      "Val loss: 0.825402, Val accuracy: 0.0037\n",
      "\n",
      "[INFO] EPOCH: 11/100\n",
      "Train loss: 2.791322, Train accuracy: 0.0704\n",
      "Val loss: 0.821787, Val accuracy: 0.0111\n",
      "\n",
      "[INFO] EPOCH: 21/100\n",
      "Train loss: 1.927035, Train accuracy: 0.2519\n",
      "Val loss: 0.690746, Val accuracy: 0.0481\n",
      "\n",
      "[INFO] EPOCH: 31/100\n",
      "Train loss: 1.528735, Train accuracy: 0.3333\n",
      "Val loss: 0.634017, Val accuracy: 0.0556\n",
      "\n",
      "[INFO] EPOCH: 41/100\n",
      "Train loss: 1.078601, Train accuracy: 0.4667\n",
      "Val loss: 0.657821, Val accuracy: 0.0556\n",
      "\n",
      "[INFO] EPOCH: 51/100\n",
      "Train loss: 0.921774, Train accuracy: 0.4926\n",
      "Val loss: 0.794817, Val accuracy: 0.0593\n",
      "\n",
      "[INFO] EPOCH: 61/100\n",
      "Train loss: 0.842958, Train accuracy: 0.5370\n",
      "Val loss: 0.817731, Val accuracy: 0.0556\n",
      "\n",
      "[INFO] EPOCH: 71/100\n",
      "Train loss: 0.687863, Train accuracy: 0.5704\n",
      "Val loss: 0.718829, Val accuracy: 0.0593\n",
      "\n",
      "[INFO] EPOCH: 81/100\n",
      "Train loss: 0.657391, Train accuracy: 0.5926\n",
      "Val loss: 0.778643, Val accuracy: 0.0556\n",
      "\n",
      "[INFO] EPOCH: 91/100\n",
      "Train loss: 0.568179, Train accuracy: 0.6074\n",
      "Val loss: 0.834987, Val accuracy: 0.0593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over our epochs\n",
    "\n",
    "for e in range(0, EPOCHS):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (x, y) in train_loader:\n",
    "\t\tx = torch.stack([image.to(device) for image in x])\n",
    "\t\ty = torch.stack([torch.LongTensor([target]) for target in y])\n",
    "\t\ty = torch.squeeze(y)\n",
    "\n",
    "\t\toutput = model(x)\n",
    "\t\tloss = criterion(output, y)\n",
    "\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\t\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (output.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "\t\t\n",
    "\tif e % 10 == 0:\n",
    "\n",
    "\t\t\"EVALUATION\"\n",
    "\n",
    "\t\t# switch off autograd for evaluation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# set the model in evaluation mode\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\t\n",
    "\t\t\t# loop over the validation set\n",
    "\t\t\tfor (x, y) in validation_loader:\n",
    "\t\t\t\tx = torch.stack([image.to(device) for image in x])\n",
    "\t\t\t\ty = torch.stack([torch.LongTensor([target]) for target in y])\n",
    "\t\t\t\ty = torch.squeeze(y)\n",
    "\t\t\t\toutput = model(x)\n",
    "\t\t\t\t\n",
    "\t\t\t\ttotalValLoss += criterion(output, y)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# calculate the number of correct predictions\n",
    "\t\t\t\tvalCorrect += (output.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "\t\t# calculate the average training and validation loss\n",
    "\t\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\t\tavgValLoss = totalValLoss / valSteps\n",
    "\t\t# calculate the training and validation accuracy\n",
    "\t\ttrainCorrect = trainCorrect / len(train_loader.dataset)\n",
    "\t\tvalCorrect = valCorrect / len(validation_loader.dataset)\n",
    "\t\t# update our training history\n",
    "\t\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\t\tH[\"train_acc\"].append(trainCorrect)\n",
    "\t\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\t\tH[\"val_acc\"].append(valCorrect)\n",
    "\n",
    "\t\t# print the model training and validation information\n",
    "\t\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\t\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\t\tavgTrainLoss, trainCorrect))\n",
    "\t\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "\t\t\tavgValLoss, valCorrect))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 100/100\n",
      "Train loss: 0.568179, Train accuracy: 170.0000\n",
      "Val loss: 0.834987, Val accuracy: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# calculate the average training and validation loss\n",
    "avgTrainLoss = totalTrainLoss / trainSteps\n",
    "avgValLoss = totalValLoss / valSteps\n",
    "# calculate the training and validation accuracy\n",
    "trainCorrect = trainCorrect / len(train_loader.dataset)\n",
    "valCorrect = valCorrect / len(validation_loader.dataset)\n",
    "# update our training history\n",
    "H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "H[\"train_acc\"].append(trainCorrect)\n",
    "H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "H[\"val_acc\"].append(valCorrect)\n",
    "\"\"\"\n",
    "# print the model training and validation information\n",
    "print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "    avgTrainLoss, trainCorrect))\n",
    "print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "    avgValLoss, valCorrect))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only saving one for now\n",
    "os.makedirs(\"classification_models\", exist_ok = True)\n",
    "\n",
    "args = {\n",
    "    \"plot_loss\": \"classification_models/plot_loss.png\",\n",
    "    \"plot_acc\": \"classification_models/plot_acc.png\",\n",
    "    \"model\": \"classification_models/model.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(len(H[\"train_loss\"]))*10\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(epochs, H[\"train_loss\"],  label=\"train_loss\")\n",
    "plt.plot(epochs, H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training and Validation Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(args[\"plot_loss\"])\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(epochs, H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(epochs, H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training and Validation Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(args[\"plot_acc\"])\n",
    "# serialize the model to disk\n",
    "torch.save(model, args[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
