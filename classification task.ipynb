{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the letters for classification \n",
    "Later should use the segmented created pages to pretrain the model\n",
    "And the actual dataset to fine tune it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management.loadHabbakuk import habbakukLettersDataset\n",
    "from data_management.datasetSplitter import datasetSplitter\n",
    "from segment import Segment\n",
    "\n",
    "AugmentedFolder = \"Data/Habbakuk_Letters\"\n",
    "\n",
    "# options on whether or not you want the labels and/or the BBs (both false also works)\n",
    "Aug_dataset = habbakukLettersDataset(folder_path=AugmentedFolder)\n",
    "BATCH_SIZE = 16\n",
    "train_loader, validation_loader = datasetSplitter(Aug_dataset, batch_size=BATCH_SIZE, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkbril2 train, test, validation sets\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from data_management.loadDSSCharacters import monkbrillDataset\n",
    "from data_management.datasetSplitter import datasetSplitter, collate_fn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "shuffle_loaders = True\n",
    "\n",
    "\n",
    "dataSource = \"monkbrill2\" # name of the folder, works with \"monkbrill\" as well\n",
    "\n",
    "train_folder = \"Data/Letters/train/\"\n",
    "val_folder = \"Data/Letters/validation/\"\n",
    "test_folder = \"Data/Letters/test/\"\n",
    "\n",
    "train_dataset = monkbrillDataset(folder_path=train_folder)\n",
    "val_dataset = monkbrillDataset(folder_path=val_folder)\n",
    "test_dataset = monkbrillDataset(folder_path=train_folder)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "validation_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=shuffle_loaders)\n",
    "\n",
    "\n",
    "characterClasses = train_dataset.characterClasses\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target = next(iter(train_loader))\n",
    "input[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later try to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#segment = Segment()\n",
    "#segment.segment_characters(real_data=True, generated_data=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arquitecture has been decided very arbitrarily for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharacterCNN(nn.Module):\n",
    "    def __init__(self, numChannels = 1, classes = 27, BATCH_SIZE = 16):\n",
    "        super(CharacterCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(numChannels, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, BATCH_SIZE, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear( BATCH_SIZE *12* 12, 128) #this shoulf be BATCH_SIZE *12* 12 but that doesnt work\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "        self.relu = nn.ReLU() # could also just use nn.functional.relu\n",
    "        #self.logSoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    " \n",
    "    # Using nn.functional provides a more concise syntax since it directly applies the operations as functions,\n",
    "    # while using the corresponding layers from torch.nn allows for explicit control over the layers used in the network\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        #x = torch.flatten(x, 1)\n",
    "        x = x.view(x.size(0), -1) #same as flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        #output = self.logSoftmax(output)\n",
    "        return output\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = CharacterCNN()\n",
    "\n",
    "# Print the model architecture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\") # ??¿¿\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# construct the argument parser and parse the arguments\\nmodel_dir = \"Model1\"\\nos.makedirs(model_dir, exist_ok = True)\\n\\nap = argparse.ArgumentParser()\\nap.add_argument(\"-m\", \"--model\", type=str, required=True,\\n\\thelp=model_dir)\\nap.add_argument(\"-p\", \"--plot\", type=str, required=True,\\n\\thelp=model_dir)\\nargs = vars(ap.parse_args())\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# construct the argument parser and parse the arguments\n",
    "model_dir = \"Model1\"\n",
    "os.makedirs(model_dir, exist_ok = True)\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-m\", \"--model\", type=str, required=True,\n",
    "\thelp=model_dir)\n",
    "ap.add_argument(\"-p\", \"--plot\", type=str, required=True,\n",
    "\thelp=model_dir)\n",
    "args = vars(ap.parse_args())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(train_loader.dataset) // BATCH_SIZE\n",
    "valSteps = len(validation_loader.dataset) // BATCH_SIZE\n",
    "\n",
    "\n",
    "# set the device we will be using to train the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the  model\n",
    "\n",
    "model = CharacterCNN().to(device)\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.NLLLoss() #When we combine the nn.NLLoss class with LogSoftmax in our model definition, we arrive at categorical cross-entropy loss\n",
    "#  (which is the equivalent to training a model with an output Linear layer and an nn.CrossEntropyLoss loss).\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "\t\"train_loss\": [],\n",
    "\t\"train_acc\": [],\n",
    "\t\"val_loss\": [],\n",
    "\t\"val_acc\": []\n",
    "}\n",
    "# measure how long training is going to take\n",
    "startTime = time.time()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/100\n",
      "Train loss: 1.436567, Train accuracy: 0.6235\n",
      "Val loss: 0.674593, Val accuracy: 0.8117\n",
      "\n",
      "[INFO] EPOCH: 11/100\n",
      "Train loss: 0.028675, Train accuracy: 0.9912\n",
      "Val loss: 0.465859, Val accuracy: 0.9020\n",
      "\n",
      "[INFO] EPOCH: 21/100\n",
      "Train loss: 0.099446, Train accuracy: 0.9687\n",
      "Val loss: 0.600194, Val accuracy: 0.8813\n",
      "\n",
      "[INFO] EPOCH: 31/100\n",
      "Train loss: 0.000143, Train accuracy: 1.0000\n",
      "Val loss: 0.651623, Val accuracy: 0.9046\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\classification task.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([torch\u001b[39m.\u001b[39mLongTensor([target]) \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m y])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# zero out the gradients, perform the backpropagation step,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# and update the weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\classification task.ipynb Cell 15\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#x = torch.flatten(x, 1)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/HandwritingRecognition-2023/classification%20task.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m#same as flatten\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\HandwritingRecognition-2023\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over our epochs\n",
    "\n",
    "for e in range(0, EPOCHS):\n",
    "\t# set the model in training mode\n",
    "\tmodel.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\t# initialize the number of correct predictions in the training\n",
    "\t# and validation step\n",
    "\ttrainCorrect = 0\n",
    "\tvalCorrect = 0\n",
    "\t# loop over the training set\n",
    "\tfor (x, y) in train_loader:\n",
    "\t\tx = torch.stack([image.to(device) for image in x])\n",
    "\t\ty = torch.stack([torch.LongTensor([target]) for target in y])\n",
    "\t\ty = torch.squeeze(y)\n",
    "\n",
    "\t\toutput = model(x)\n",
    "\t\tloss = criterion(output, y)\n",
    "\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\t\n",
    "\t\t# zero out the gradients, perform the backpropagation step,\n",
    "\t\t# and update the weights\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far and\n",
    "\t\t# calculate the number of correct predictions\n",
    "\t\ttotalTrainLoss += loss\n",
    "\t\ttrainCorrect += (output.argmax(1) == y).type(\n",
    "\t\t\ttorch.float).sum().item()\n",
    "\n",
    "\t\t\n",
    "\tif e % 10 == 0:\n",
    "\n",
    "\t\t\"EVALUATION\"\n",
    "\n",
    "\t\t# switch off autograd for evaluation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# set the model in evaluation mode\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\t\n",
    "\t\t\t# loop over the validation set\n",
    "\t\t\tfor (x, y) in validation_loader:\n",
    "\t\t\t\tx = torch.stack([image.to(device) for image in x])\n",
    "\t\t\t\ty = torch.stack([torch.LongTensor([target]) for target in y])\n",
    "\t\t\t\ty = torch.squeeze(y)\n",
    "\t\t\t\toutput = model(x)\n",
    "\t\t\t\t\n",
    "\t\t\t\ttotalValLoss += criterion(output, y)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# calculate the number of correct predictions\n",
    "\t\t\t\tvalCorrect += (output.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "\t\t# calculate the average training and validation loss\n",
    "\t\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\t\tavgValLoss = totalValLoss / valSteps\n",
    "\t\t# calculate the training and validation accuracy\n",
    "\t\ttrainCorrect = trainCorrect / len(train_loader.dataset)\n",
    "\t\tvalCorrect = valCorrect / len(validation_loader.dataset)\n",
    "\t\t# update our training history\n",
    "\t\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\t\tH[\"train_acc\"].append(trainCorrect)\n",
    "\t\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\t\tH[\"val_acc\"].append(valCorrect)\n",
    "\n",
    "\t\t# print the model training and validation information\n",
    "\t\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "\t\tprint(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "\t\t\tavgTrainLoss, trainCorrect))\n",
    "\t\tprint(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "\t\t\tavgValLoss, valCorrect))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 37/100\n",
      "Train loss: 0.000143, Train accuracy: 2400.0000\n",
      "Val loss: 0.651623, Val accuracy: 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# calculate the average training and validation loss\n",
    "avgTrainLoss = totalTrainLoss / trainSteps\n",
    "avgValLoss = totalValLoss / valSteps\n",
    "# calculate the training and validation accuracy\n",
    "trainCorrect = trainCorrect / len(train_loader.dataset)\n",
    "valCorrect = valCorrect / len(validation_loader.dataset)\n",
    "# update our training history\n",
    "H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "H[\"train_acc\"].append(trainCorrect)\n",
    "H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "H[\"val_acc\"].append(valCorrect)\n",
    "\"\"\"\n",
    "# print the model training and validation information\n",
    "print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "    avgTrainLoss, trainCorrect))\n",
    "print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "    avgValLoss, valCorrect))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only saving one for now\n",
    "os.makedirs(\"classification_models\", exist_ok = True)\n",
    "\n",
    "args = {\n",
    "    \"plot_loss\": \"classification_models/plot_loss.png\",\n",
    "    \"plot_acc\": \"classification_models/plot_acc.png\",\n",
    "    \"model\": \"classification_models/model.pth\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "import numpy as np\n",
    "\n",
    "epochs = np.arange(len(H[\"train_loss\"]))*10\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(epochs, H[\"train_loss\"],  label=\"train_loss\")\n",
    "plt.plot(epochs, H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training and Validation Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(args[\"plot_loss\"])\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(epochs, H[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(epochs, H[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training and Validation Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(args[\"plot_acc\"])\n",
    "# serialize the model to disk\n",
    "torch.save(model, args[\"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
